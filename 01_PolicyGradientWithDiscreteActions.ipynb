{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ScN5VD0jo--"
      },
      "source": [
        "# Policy Gradient with Discrete Action Space\n",
        "**Author:** Marc-Philip Ecker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsVnwq9kH-H0"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCri9IplH9Ep"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVtbcn_2Jhs-"
      },
      "source": [
        "## Environment\n",
        "In this exercise we develop an RL-agent for the gymnasium cartpole environemen (see https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vmY3GMDJlWr",
        "outputId": "caa48b36-509b-45b9-b238-8d60c34fc44a"
      },
      "outputs": [],
      "source": [
        "env_name = \"CartPole-v1\"\n",
        "env = gym.make(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMjuHzgGJJJ7"
      },
      "source": [
        "## Define the Policy\n",
        "In this section we defining the policy as a neural network. The policy $\\pi_{\\boldsymbol{\\theta}}(\\mathbf{u}|\\mathbf{x})$ defines the probabilities of taking an action $\\mathbf{u}\\in\\mathcal{U}$ given that the environment is in a certain state $\\mathbf{x}\\in\\mathcal{X}$. The action space for the gymnasium CartPole-v1 environment is discrete, i.e. $\\mathcal{U}=\\{\\mathbf{u}_1,\\mathbf{u}_2\\}$. We create a neural network that takes as input the current state $\\mathbf{x}\\in\\mathcal{X}$ and returns the unnormalized log probabilities $\\log\\pi_{\\boldsymbol{\\theta}}(\\mathbf{u}|\\mathbf{x})$ (logits). Since, $\\mathcal{X}\\subseteq\\mathbb{R}^n$, the layer requires $n$ input units to handle the state as input. We use one hidden layer with 32 units and $m=2$ outputs, which represent the log probabilities.\n",
        "\n",
        "Hence, the network is defined by the parameters $\\boldsymbol{\\theta}=\\{\\mathbf{W}_1,\\mathbf{b}_1,\\mathbf{W}_2,\\mathbf{b}_2\\}$, where $\\mathbf W_1\\in\\mathbb{R}^{n\\times 32}$, $\\mathbf{W}_2\\in\\mathbb{R}^{32\\times m}$ are the weighting matrices of the linear units and $\\mathbf{b}_1\\in\\mathbb{R}^{32}$, $\\mathbf{b}_2\\in\\mathbb{R}^{m}$ are the bias vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmMevJB0KuEn"
      },
      "outputs": [],
      "source": [
        "# Get dimensions of state space and action space from environment\n",
        "n = env.observation_space.shape[0];\n",
        "m = env.action_space.n;\n",
        "\n",
        "# Define layers sizes\n",
        "layer_sizes = [n, 32, 32, m]\n",
        "\n",
        "# Set layers\n",
        "layers = []\n",
        "for i in range(len(layer_sizes)-1):\n",
        "  act = nn.Tanh if i < len(layer_sizes) - 2 else nn.Identity\n",
        "  layers += [nn.Linear(layer_sizes[i], layer_sizes[i+1]), act()]\n",
        "\n",
        "# Define policy\n",
        "policy_network = nn.Sequential(*layers)\n",
        "\n",
        "# Reset network parameters\n",
        "def reset_policy():\n",
        "  for layer in policy_network.children():\n",
        "    if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXWXqdD2MyuT"
      },
      "source": [
        "We define a distribution that takes the outputs of the neural network as input. In this case we use the Categorical distribution (https://pytorch.org/docs/stable/distributions.html#categorical) which either takes as input the log probabilities or the probabilities. Since our network returns the log probabilities, we use these as input. The probabilities are then computed using the Softmax function\n",
        "\\begin{align}\n",
        "  \\pi_{\\boldsymbol{\\theta}}(\\mathbf{u}_i|\\mathbf{x})=\\frac{\\exp(\\log\\pi_{\\boldsymbol{\\theta}}(\\mathbf{u}_i|\\mathbf{x}))}{\\sum_{i=1}^2\\exp(\\log\\pi_{\\boldsymbol{\\theta}}(\\mathbf{u}_i|\\mathbf{x}))}\\ .\n",
        "\\end{align}\n",
        "We can use this distribution to generate samples from our policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w-pA-LNPZTK"
      },
      "outputs": [],
      "source": [
        "def get_policy(x):\n",
        "  logits = policy_network(x)\n",
        "  return Categorical(logits=logits)\n",
        "\n",
        "def sample_action(x):\n",
        "  return get_policy(x).sample().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_policy():\n",
        "  env_local = gym.make(env_name, render_mode=\"human\")\n",
        "  x = env_local.reset()[0]\n",
        "  while True:\n",
        "    env_local.render()\n",
        "    u = sample_action(torch.as_tensor(x, dtype=torch.float32))\n",
        "    x, _, done, _, _ = env_local.step(u)\n",
        "\n",
        "    if done:\n",
        "      x = env_local.reset()[0]\n",
        "      break\n",
        "    \n",
        "  env_local.close()\n",
        "\n",
        "visualize_policy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhyaFuUdQutd"
      },
      "source": [
        "## Policy Gradient\n",
        "\n",
        "In RL, the goal is to maximize the sum of rewards\n",
        "\\begin{align}\n",
        "  \\max_{\\boldsymbol\\theta}J(\\boldsymbol{\\theta})=\\mathbb{E}_{\\boldsymbol{\\tau}\\sim p_{\\boldsymbol{\\theta}}}\\Big\\{\\sum_{k=0}^{N-1}r(\\mathbf{x}_k,\\mathbf{u}_k)\\Big\\}\\ .\n",
        "\\end{align}\n",
        "Given a set of i.i.d. sample trajectories $\\boldsymbol{\\tau}^{[i]}=\\{\\mathbf{x}_0^{[i]},\\mathbf{u}_0^{[i]}, \\dots,\\mathbf{x}_{N-1}^{[i]},\\mathbf{u}_{N-1}^{[i]}\\}$, $i=1,\\dots,N_{\\mathrm{sample}}$, an unbiased estimator of the expectation can be computed\n",
        "\\begin{align}\n",
        "  J(\\boldsymbol{\\theta})\\approx\\frac{1}{N_{\\mathrm{sample}}}\\sum_{i=1}^{N_{\\mathrm{sample}}}\\sum_{k=0}^{N-1}r(\\mathbf{x}_k,\\mathbf{u}_k)\\ .\n",
        "\\end{align}\n",
        "The goal of policy gradient is to adapt parameters of the policy s.t. the expected return increases. This is achieved using gradient ascent $\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}+\\alpha\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$. The policy gradient is given by\n",
        "\\begin{align}\n",
        "  \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})=\\mathbb{E}_{\\boldsymbol{\\tau}\\sim p_{\\boldsymbol{\\theta}}}\\Big\\{\\sum_{k=0}^{N-1}\\nabla_{\\boldsymbol{\\theta}}\\log\\pi_{\\boldsymbol{\\theta}}(\\mathbf{u}_k|\\mathbf{x}_k)\\sum_{k=0}^{N-1}r(\\mathbf{x}_k,\\mathbf{u}_k)\\Big\\}\n",
        "\\end{align}\n",
        "and it can be estimated using\n",
        "\\begin{align}\n",
        "  \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})\\approx\\frac{1}{N_{\\mathrm{sample}}}\\sum_{i=1}^{N_{\\mathrm{sample}}}\\Big\\{\\sum_{k=0}^{N-1}\\nabla_{\\boldsymbol{\\theta}}\\log\\pi_{\\boldsymbol{\\theta}}(\\mathbf{u}_k^{[i]}|\\mathbf{x}_k^{[i]})\\sum_{k=0}^{N-1}r(\\mathbf{x}_k^{[i]},\\mathbf{u}_k^{[i]})\\Big\\}\\ .\n",
        "\\end{align}\n",
        "\n",
        "Using the loss function\n",
        "\\begin{align}\n",
        "  L(\\boldsymbol{\\theta})=-\\frac{1}{N_{\\mathrm{sample}}}\\sum_{i=1}^{N_{\\mathrm{sample}}}\\Big\\{\\sum_{k=0}^{N-1}\\log\\pi_{\\boldsymbol{\\theta}}(\\mathbf{u}_k^{[i]}|\\mathbf{x}_k^{[i]})\\sum_{k=0}^{N-1}r(\\mathbf{x}_k^{[i]},\\mathbf{u}_k^{[i]})\\Big\\}\n",
        "\\end{align}\n",
        "allows to use the autodiff functionality of PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-k5wja1QxXd"
      },
      "outputs": [],
      "source": [
        "def get_loss(x, u, R):\n",
        "  log_pi = get_policy(x).log_prob(u)\n",
        "  return -(log_pi * R).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TwryOqvli1_"
      },
      "source": [
        "The main algorthm works as follows:\n",
        "\n",
        "For each epoch:\n",
        "\n",
        "1.   Generate sample trajectories $\\boldsymbol{\\tau}^{[i]}$, $i=1,\\dots,N_{\\mathrm{sample}}$ and the corresponding returns $R(\\boldsymbol{\\tau})=\\sum_{k=0}^{N-1}r(\\mathbf{x}_k^{[i]},\\mathbf{u}_k^{[i]})$\n",
        "2.   Compute the policy gradient $\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$. This is done by PyTorch backprop.\n",
        "3.   Perform gradient ascent step $\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}+\\alpha\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$ (PyTorch optimizer.step())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc8HYNtLRHl8"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(optimizer, batch_size):\n",
        "  x_batch = []\n",
        "  u_batch = []\n",
        "  R_batch = []\n",
        "  w_batch = []\n",
        "  N_batch = []\n",
        "\n",
        "  x = env.reset()[0]\n",
        "  rewards = []\n",
        "\n",
        "  # Sample trajectories\n",
        "  while True:\n",
        "    # Sample action\n",
        "    u = sample_action(torch.as_tensor(x, dtype=torch.float32))\n",
        "    # Store state action pair\n",
        "    x_batch.append(x)\n",
        "    u_batch.append(u)\n",
        "    # Apply action to environment\n",
        "    x, r, done, _, _ = env.step(u)\n",
        "    # Store reward\n",
        "    rewards.append(r)\n",
        "\n",
        "    if done:\n",
        "      # Compute return and episode length\n",
        "      R = sum(rewards)\n",
        "      N = len(rewards)\n",
        "      # Store return and episode length\n",
        "      R_batch.append(R)\n",
        "      N_batch.append(N)\n",
        "      # Compute weights for loss. There are N terms that are weighted\n",
        "      # by the return\n",
        "      w_batch += [R] * N\n",
        "\n",
        "      x = env.reset()[0]\n",
        "      done = False\n",
        "      rewards = []\n",
        "\n",
        "      if len(x_batch) > batch_size:\n",
        "        break\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  # Define the loss for the current batch\n",
        "  batch_loss = get_loss(torch.as_tensor(x_batch, dtype=torch.float32),\n",
        "                        torch.as_tensor(u_batch, dtype=torch.float32),\n",
        "                        torch.as_tensor(w_batch, dtype=torch.float32))\n",
        "  # Compute gradient\n",
        "  batch_loss.backward()\n",
        "  # Make a gradient ascent step\n",
        "  optimizer.step()\n",
        "\n",
        "  return batch_loss, R_batch, N_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJBIXPHvTtfY"
      },
      "outputs": [],
      "source": [
        "def train(epochs=50, batch_size=5000, learning_rate=1e-2):\n",
        "  optimizer = Adam(policy_network.parameters(), learning_rate)\n",
        "\n",
        "  returns = []\n",
        "  for i in range(epochs):\n",
        "    batch_loss, R_batch, N_batch = train_one_epoch(optimizer, batch_size)\n",
        "    returns.append(np.mean(R_batch))\n",
        "    print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t episode length: %.3f'%\n",
        "                (i, batch_loss, np.mean(R_batch), np.mean(N_batch)))\n",
        "  return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6KAaX7ZVTNY",
        "outputId": "eb038017-1429-4025-d45a-e85835306098"
      },
      "outputs": [],
      "source": [
        "reset_policy()\n",
        "returns = train()\n",
        "visualize_policy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Causality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_reward_to_go(rewards):\n",
        "  N = len(rewards)\n",
        "  r_to_go = [0] * N\n",
        "  for i in reversed(range(N)):\n",
        "    r_to_go[i] = rewards[i] + (r_to_go[i+1] if i < N-1 else 0)\n",
        "\n",
        "  return r_to_go\n",
        "\n",
        "def train_one_epoch(optimizer, batch_size):\n",
        "  x_batch = []\n",
        "  u_batch = []\n",
        "  R_batch = []\n",
        "  w_batch = []\n",
        "  N_batch = []\n",
        "\n",
        "  x = env.reset()[0]\n",
        "  rewards = []\n",
        "\n",
        "  # Sample trajectories\n",
        "  while True:\n",
        "    # Sample action\n",
        "    u = sample_action(torch.as_tensor(x, dtype=torch.float32))\n",
        "    # Store state action pair\n",
        "    x_batch.append(x)\n",
        "    u_batch.append(u)\n",
        "    # Apply action to environment\n",
        "    x, r, done, _, _ = env.step(u)\n",
        "    # Store reward\n",
        "    rewards.append(r)\n",
        "\n",
        "    if done:\n",
        "      # Compute return and episode length\n",
        "      R = sum(rewards)\n",
        "      N = len(rewards)\n",
        "      # Store return and episode length\n",
        "      R_batch.append(R)\n",
        "      N_batch.append(N)\n",
        "      # Compute weights for loss. There are N terms that are weighted\n",
        "      # by the return\n",
        "      w_batch += get_reward_to_go(rewards)\n",
        "\n",
        "      x = env.reset()[0]\n",
        "      done = False\n",
        "      rewards = []\n",
        "\n",
        "      if len(x_batch) > batch_size:\n",
        "        break\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  # Define the loss for the current batch\n",
        "  batch_loss = get_loss(torch.as_tensor(x_batch, dtype=torch.float32),\n",
        "                        torch.as_tensor(u_batch, dtype=torch.float32),\n",
        "                        torch.as_tensor(w_batch, dtype=torch.float32))\n",
        "  # Compute gradient\n",
        "  batch_loss.backward()\n",
        "  # Make a gradient ascent step\n",
        "  optimizer.step()\n",
        "\n",
        "  return batch_loss, R_batch, N_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reset_policy()\n",
        "returns_causality = train()\n",
        "visualize_policy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(optimizer, batch_size):\n",
        "  x_batch = []\n",
        "  u_batch = []\n",
        "  R_batch = []\n",
        "  w_batch = []\n",
        "  N_batch = []\n",
        "\n",
        "  x = env.reset()[0]\n",
        "  rewards = []\n",
        "\n",
        "  # Sample trajectories\n",
        "  while True:\n",
        "    # Sample action\n",
        "    u = sample_action(torch.as_tensor(x, dtype=torch.float32))\n",
        "    # Store state action pair\n",
        "    x_batch.append(x)\n",
        "    u_batch.append(u)\n",
        "    # Apply action to environment\n",
        "    x, r, done, _, _ = env.step(u)\n",
        "    # Store reward\n",
        "    rewards.append(r)\n",
        "\n",
        "    if done:\n",
        "      # Compute return and episode length\n",
        "      R = sum(rewards)\n",
        "      N = len(rewards)\n",
        "      # Store return and episode length\n",
        "      R_batch.append(R)\n",
        "      N_batch.append(N)\n",
        "      # Compute weights for loss. There are N terms that are weighted\n",
        "      # by the return\n",
        "      w_batch += [R] * N\n",
        "\n",
        "      x = env.reset()[0]\n",
        "      done = False\n",
        "      rewards = []\n",
        "\n",
        "      if len(x_batch) > batch_size:\n",
        "        # Baseline\n",
        "        R_mean = np.mean(R_batch)\n",
        "        for i in range(len(w_batch)):\n",
        "          w_batch[i] -= R_mean\n",
        "        break\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  # Define the loss for the current batch\n",
        "  batch_loss = get_loss(torch.as_tensor(x_batch, dtype=torch.float32),\n",
        "                        torch.as_tensor(u_batch, dtype=torch.float32),\n",
        "                        torch.as_tensor(w_batch, dtype=torch.float32))\n",
        "  # Compute gradient\n",
        "  batch_loss.backward()\n",
        "  # Make a gradient ascent step\n",
        "  optimizer.step()\n",
        "\n",
        "  return batch_loss, R_batch, N_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reset_policy()\n",
        "returns_baseline = train()\n",
        "visualize_policy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(returns)\n",
        "plt.plot(returns_causality)\n",
        "plt.plot(returns_baseline)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
