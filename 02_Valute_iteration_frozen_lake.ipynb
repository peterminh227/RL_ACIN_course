{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FROZEN LAKE\n",
    "\n",
    "https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n",
    "\n",
    "### Action Space\n",
    "The action shape is (1,) in the range {0, 3} indicating which direction to move the player.\n",
    "```\n",
    "0: Move left\n",
    "\n",
    "1: Move down\n",
    "\n",
    "2: Move right\n",
    "\n",
    "3: Move up\n",
    "```\n",
    "### Observation Space\n",
    "The observation is a value representing the playerâ€™s current position as current_row * nrows + current_col (where both the row and col start at 0).\n",
    "\n",
    "For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map.\n",
    "\n",
    "The observation is returned as an int().\n",
    "\n",
    "Starting State\n",
    "The episode starts with the player in state [0] (location [0, 0]).\n",
    "\n",
    "### Rewards\n",
    "Reward schedule:\n",
    "```\n",
    "Reach goal: +1\n",
    "\n",
    "Reach hole: 0\n",
    "\n",
    "Reach frozen: 0\n",
    "```\n",
    "### Episode End\n",
    "The episode ends if the following happens:\n",
    "```\n",
    "Termination:\n",
    "\n",
    "The player moves into a hole.\n",
    "\n",
    "The player reaches the goal at max(nrow) * max(ncol) - 1 (location [max(nrow)-1, max(ncol)-1]).\n",
    "\n",
    "Truncation (when using the time_limit wrapper):\n",
    "\n",
    "The length of the episode is 100 for 4x4 environment, 200 for FrozenLake8x8-v1 environment.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment id:  FrozenLake-v1\n",
      "Number of action space:  Discrete(4)\n",
      "Number of observation space:  Discrete(16)\n",
      "How to move from (S) --> (G) and avoiding (H) with the below map\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "env_id = 'FrozenLake-v1'\n",
    "fl_env = gym.make(env_id, desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"ansi\")\n",
    "print(\"Environment id: \", env_id)\n",
    "print(\"Number of action space: \",fl_env.env.action_space)\n",
    "print(\"Number of observation space: \",fl_env.env.observation_space)\n",
    "'''\n",
    "SHOWING ENV\n",
    "'''\n",
    "fl_env.reset()\n",
    "print(\"How to move from (S) --> (G) and avoiding (H) with the below map\")\n",
    "print(fl_env.render())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' list's structure\n",
    "fl_env.p[s|a] = p[s_prime],s_prime,r, done\n",
    "probalitity of reaching (s_prime) from (s) and its reward (r) by action (a)\n",
    "'''\n",
    "def argmax(env, V, pi,action_star, state, gamma):\n",
    "    Expectation_value = np.zeros(env.env.action_space.n)\n",
    "    for action in range(env.env.action_space.n):        \n",
    "        q   =   0\n",
    "        P   =   np.array(env.env.P[state][action])                   \n",
    "        (x,y) = np.shape(P)                             \n",
    "        for i in range(x):                              \n",
    "            s_prime                   = int(P[i][1])                      \n",
    "            p                         = P[i][0]\n",
    "            r                         = P[i][2]                                 \n",
    "            q                         += p*(r+gamma*V[s_prime])          \n",
    "            Expectation_value[action] = q\n",
    "\n",
    "    chosen_action   = np.argmax(Expectation_value) \n",
    "    action_star[state]     =  chosen_action\n",
    "    pi[state][chosen_action]    = 1\n",
    "    return pi, action_star    \n",
    "''''\n",
    "bellman_optimality_update \n",
    "'''\n",
    "def bellman_optimality_update(env, V, state, gamma):   \n",
    "    '''\n",
    "    '''\n",
    "    pi                = np.zeros((env.env.observation_space.n, env.env.action_space.n))\n",
    "    Expectation_value = np.zeros(env.env.action_space.n)                       \n",
    "                                            \n",
    "    for action in range(env.env.action_space.n):             \n",
    "        q       = 0                                 \n",
    "        P       = np.array(env.env.P[state][action])\n",
    "        (x,y)   = np.shape(P)\n",
    "        \n",
    "        for i in range(x):\n",
    "            s_prime                     = int(P[i][1])\n",
    "            p                           = P[i][0]\n",
    "            r                           = P[i][2]\n",
    "            q                           += p*(r + gamma*V[s_prime])\n",
    "            Expectation_value[action]   = q\n",
    "            \n",
    "    chosen_action = np.argmax(Expectation_value)\n",
    "    pi[state][chosen_action]      = 1\n",
    "    \n",
    "    ## Taking greedy action and update value function\n",
    "    u = 0\n",
    "    P = np.array(env.env.P[state][chosen_action])\n",
    "    (x,y) = np.shape(P)\n",
    "    for i in range(x):\n",
    "        s_prime = int(P[i][1])\n",
    "        p       = P[i][0]\n",
    "        r       = P[i][2]\n",
    "        u       += p*(r + gamma*V[s_prime])\n",
    "        \n",
    "    V[state] =  u\n",
    "    return V[state]\n",
    "''''\n",
    "VALUE_ITERATION \n",
    "'''\n",
    "def value_iteration(env, gamma, theta):\n",
    "    V = np.zeros(env.env.observation_space.n)       \n",
    "    num_steps = 0                              \n",
    "    while True:\n",
    "        num_steps += 1\n",
    "        delta = 0\n",
    "        for state in range(env.env.observation_space.n):                     \n",
    "            v = V[state]\n",
    "            bellman_optimality_update(env, V, state, gamma)   \n",
    "            delta = max(delta, abs(v - V[state])) \n",
    "        if delta < theta:                                       \n",
    "            break                                         \n",
    "    pi = np.zeros((env.env.observation_space.n, env.env.action_space.n)) \n",
    "\n",
    "    action_star = np.zeros((env.env.observation_space.n))\n",
    "\n",
    "    for state in range(env.env.observation_space.n):\n",
    "        pi,action_star = argmax(env, V, pi,action_star, state, gamma)         \n",
    "    return V, pi, action_star                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''EXECUTION\n",
    "'''                                 \n",
    "gamma = 0.9\n",
    "theta = 1e-4\n",
    "V, policy,action_star = value_iteration(fl_env, gamma,  theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Playing game\n",
    "'''\n",
    "current_state = fl_env.reset()[0]\n",
    "print(fl_env.render())\n",
    "for i in range(100):\n",
    "    current_state, reward, done,_, info = fl_env.step( int(action_star[current_state]))\n",
    "    print(fl_env.render())\n",
    "    if done:\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
